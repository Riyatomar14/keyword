# Network Flow: Physical NIC â†’ Virtual NIC â†’  Container
---

## ðŸŽ¯ What We're Learning

**How does a network packet travel from a physical cable all the way into a container?**

Let me show you with clear diagrams and simple explanations.

---

## ðŸ“Š THE COMPLETE PICTURE

```                 
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  1. PHYSICAL WORLD                                      â”‚
â”‚     [Ethernet Cable] â”€â”€> [Physical NIC Card]           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ IRQ (Interrupt Request)
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  2. LINUX KERNEL                                        â”‚
â”‚     Driver reads packet â†’ creates sk_buff              â”‚
â”‚     Packet is now "inside" the operating system        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Now packet must choose a path...
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚            â”‚
        â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
    â”‚ PP  â”‚     â”‚ PB  â”‚     â”‚ PF  â”‚
    â”‚OVS  â”‚     â”‚DPDK â”‚     â”‚eBPF â”‚
    â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜
       â”‚           â”‚            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  3. VIRTUAL NIC (veth pair)                             â”‚
â”‚     Host Side â†â”€â”€cableâ”€â”€â†’ Container Side                â”‚
â”‚     [veth0]   â†â”€linkedâ”€â†’  [veth1 = eth0]                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  4. SOFTWARE SWITCH (Bridge or OVS)                     â”‚
â”‚     Decides: which veth should get this packet?         â”‚
â”‚     Like a traffic cop directing cars                   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  5. CNI PLUGIN (Cilium/Calico/Flannel)                â”‚
â”‚     This is WHO set everything up in the first place   â”‚
â”‚     Created the veth, assigned IPs, configured routing â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  6. CONTAINER / POD                                     â”‚
â”‚     Packet arrives! Container sees it on "eth0"        â”‚
â”‚     Application receives the data                      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ” DETAILED EXPLANATION OF EACH LAYER

---

## 1ï¸âƒ£ PHYSICAL NIC - The Hardware Gateway

```
     Real World Network
            |
            | (Ethernet cable)
            |
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Physical NIC â”‚  â† Real hardware chip
    â”‚   (eth0)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ (writes to RAM via DMA)
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  DMA Buffer   â”‚  â† Special memory area
    â”‚   in RAM      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ (fires IRQ - interrupt signal)
            â”‚
            â–¼
```

### **What is happening here:**

**Physical NIC** = A real hardware card plugged into your server
- Has a MAC address (like `00:1A:2B:3C:4D:5E`)
- Connected to actual cables
- Receives electrical signals or light pulses

**What it does:**
1. Frame arrives on the wire
2. NIC writes it directly to RAM (DMA = Direct Memory Access, no CPU needed)
3. NIC sends an **interrupt** to the CPU: "Hey! New packet arrived!"

**Why this matters:**
- This is the ONLY real hardware in the entire chain
- Everything else is software pretending to be hardware
- All virtual NICs ultimately share THIS one physical card

---

## 2ï¸âƒ£ LINUX KERNEL - Packet Enters Software

```
            IRQ fires
               |
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Interrupt Handler â”‚
    â”‚   (CPU wakes up)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   NIC Driver        â”‚  â† Software that talks to hardware
    â”‚   (e1000, ixgbe)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Reads from DMA buffer
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Creates sk_buff   â”‚  â† The kernel's packet container
    â”‚   (socket buffer)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ netif_receive_skb() â”‚  â† Entry to network stack
    â”‚                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      Packet is now in the kernel!
```

### **What is happening here:**

**sk_buff** = The kernel's way of storing a network packet
- Contains the raw packet bytes
- Plus metadata: which interface it came from, timestamp, etc.

**Where in the kernel:**
- Driver code: `/drivers/net/ethernet/`
- Network stack: `/net/core/dev.c`

**Key point:**
From this moment on, the packet exists as a **software object** inside Linux kernel memory.

---

## 3ï¸âƒ£ THREE POSSIBLE PATHS (PP, PB, PF)

```
                  Packet is in kernel
                         |
                         |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   PP   â”‚       â”‚   PB   â”‚      â”‚   PF   â”‚
   â”‚ Packet â”‚       â”‚ Packet â”‚      â”‚ Packet â”‚
   â”‚Process â”‚       â”‚ Bypass â”‚      â”‚ Filter â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                â”‚
       â”‚                â”‚                â”‚
       â–¼                â–¼                â–¼
   Full kernel     Skip kernel      eBPF programs
   network stack   (DPDK)           (fast filtering)
```

### **Path 1: PP (Packet Processing) - The Normal Route**

```
sk_buff
  â”‚
  â”œâ”€> netfilter (iptables rules)
  â”‚
  â”œâ”€> routing decision
  â”‚
  â”œâ”€> bridge forwarding
  â”‚
  â””â”€> socket or output interface
```

**What it is:**
The default, normal path through the entire Linux network stack.

**When used:**
- Regular Linux networking
- Basic Docker setups
- Simple container clusters

**Speed:** Good, but not the fastest

---

### **Path 2: PB (Packet Bypass - DPDK) - The Fast Lane**

```
Physical NIC
     â”‚
     â”‚ (skip the kernel entirely!)
     â”‚
     â””â”€â”€> DPDK driver
            â”‚
            â””â”€â”€> User-space application
                 (no kernel involvement)
```

**What it is:**
The application reads packets DIRECTLY from the NIC's memory.
No kernel. No interrupts. No sk_buff.

**When used:**
- High-frequency trading
- Telecom / 5G
- When you need 10+ million packets/second

**Speed:** FASTEST possible

**Trade-off:** You must write your own network stack

---

### **Path 3: PF (Packet Filter - eBPF) - The Smart Path**

```
Packet arrives
     â”‚
     â”œâ”€> eBPF program at XDP (earliest point)
     â”‚   â”‚
     â”‚   â””â”€> Can drop, modify, or redirect immediately
     â”‚
     â”œâ”€> eBPF program at TC (traffic control)
     â”‚   â”‚
     â”‚   â””â”€> Can enforce policies, do NAT
     â”‚
     â””â”€> Continue to destination
```

**What it is:**
Small programs that run INSIDE the kernel at specific hook points.

**When used:**
- **Cilium** (modern CNI plugin)
- High-performance firewalling
- When you need both speed AND flexibility

**Speed:** Nearly as fast as DPDK, but stays in kernel

---

## 4ï¸âƒ£ VIRTUAL NIC (veth pair) - The Magic Cable

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HOST NAMESPACE         â”‚      â”‚   CONTAINER NAMESPACE    â”‚
â”‚   (root network stack)   â”‚      â”‚   (isolated network)     â”‚
â”‚                          â”‚      â”‚                          â”‚
â”‚         veth0            â”‚      â”‚          veth1           â”‚
â”‚           â”‚              â”‚      â”‚            â”‚             â”‚
â”‚    (host side of         â”‚      â”‚     (container side)     â”‚
â”‚     virtual cable)       â”‚      â”‚                          â”‚
â”‚           â”‚              â”‚      â”‚     Renamed to: eth0     â”‚
â”‚           â”‚              â”‚      â”‚                          â”‚
â”‚  Attached to bridge/OVS  â”‚      â”‚     Has Pod IP           â”‚
â”‚           â”‚              â”‚      â”‚     10.244.1.5/24        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                   â”‚
            â”‚    Virtual "cable" connection     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                      (anything in veth0 
                       comes out veth1)
```

### **What is a veth pair:**

Think of it as a **virtual Ethernet cable** with two ends:
- **veth0** stays on the host
- **veth1** goes into the container

**Key concept:**
When you send a packet into `veth0`, it **magically appears** at `veth1`.

### **Why we need it:**

Containers run in isolated **network namespaces**:
- Each namespace = separate network stack
- Separate interfaces, IPs, routing tables
- Totally isolated from each other

The veth pair is the **bridge** between host and container namespaces.

---

### **What is a Network Namespace:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HOST MACHINE                        â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Namespace 1    â”‚         â”‚  Namespace 2    â”‚      â”‚
â”‚  â”‚  (Container A)  â”‚         â”‚  (Container B)  â”‚      â”‚
â”‚  â”‚                 â”‚         â”‚                 â”‚      â”‚
â”‚  â”‚  eth0: 10.1.1.2 â”‚         â”‚  eth0: 10.1.1.3 â”‚      â”‚
â”‚  â”‚  routing table  â”‚         â”‚  routing table  â”‚      â”‚
â”‚  â”‚  iptables       â”‚         â”‚  iptables       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚        Root Namespace (Host)                â”‚       â”‚
â”‚  â”‚        eth0: physical NIC                   â”‚       â”‚
â”‚  â”‚        br0: bridge                          â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Network Namespace** = A completely isolated copy of the network stack.

Each container gets its own namespace:
- Can't see other containers' interfaces
- Can't conflict on IP addresses
- Total isolation

---

## 5ï¸âƒ£ BRIDGE / OVS - The Software Switch

```
                    Linux Bridge (br0)
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚               â”‚
           â–¼               â–¼               â–¼
        veth0           veth2           veth4
           â”‚               â”‚               â”‚
           â”‚               â”‚               â”‚
      (to Pod A)      (to Pod B)      (to Pod C)
```

### **What is a Bridge:**

A **Layer 2 switch** implemented in software.

**What it does:**
1. Receives frames from attached interfaces (veth ports, physical NICs)
2. Looks at destination MAC address
3. Forwards frame to the correct port

**It's like a traffic cop** directing packets to the right destination.

---

### **Linux Bridge vs OVS (Open vSwitch):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Linux Bridge    â”‚              â”‚       OVS        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  â”‚              â”‚                  â”‚
â”‚ â€¢ Simple         â”‚              â”‚ â€¢ Programmable   â”‚
â”‚ â€¢ Built-in       â”‚              â”‚ â€¢ OpenFlow       â”‚
â”‚ â€¢ Good for basic â”‚              â”‚ â€¢ SDN-ready      â”‚
â”‚   setups         â”‚              â”‚ â€¢ High perf      â”‚
â”‚                  â”‚              â”‚   (with DPDK)    â”‚
â”‚ â€¢ Used by:       â”‚              â”‚ â€¢ Used by:       â”‚
â”‚   Flannel        â”‚              â”‚   OpenStack      â”‚
â”‚   Simple Docker  â”‚              â”‚   Large clouds   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Linux Bridge:**
- Simple software switch in the kernel
- MAC learning, forwarding
- Good enough for most cases

**OVS (Open vSwitch):**
- Advanced, programmable switch
- Can write custom forwarding rules (OpenFlow)
- Can use DPDK for extreme performance
- Used in large-scale deployments

---

## 6ï¸âƒ£ CNI - The Setup Worker

```
                    Kubernetes creates a Pod
                            â”‚
                            â–¼
                    kubelet calls CNI plugin
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚               â”‚               â”‚
            â–¼               â–¼               â–¼
        Flannel         Calico          Cilium
        
        Each CNI plugin's job:
        
        1. Create network namespace
        2. Create veth pair
        3. Move one end into namespace
        4. Attach other end to bridge
        5. Assign IP address (IPAM)
        6. Set up routing rules
```

### **What is CNI:**

**CNI = Container Network Interface**

It's a **standard specification** that says:

> "When you create a container, here's how you should set up its networking."

### **Why CNI exists:**

**Kubernetes does NOT do networking itself.**

Instead, it says:
> "Hey CNI plugin, set up networking for this Pod. I don't care HOW you do it, just make it work."

This is genius because:
- Different environments need different networking
- Cloud providers can write their own plugins
- You can swap networking solutions without changing Kubernetes

---

### **What does a CNI plugin actually do:**

When a Pod is created:

```
Step 1: Create network namespace
        â””â”€> Isolated network environment for the Pod

Step 2: Create veth pair
        â””â”€> Virtual cable with two ends

Step 3: Move veth1 into Pod's namespace
        â””â”€> Now the Pod has its own network interface

Step 4: Attach veth0 to bridge
        â””â”€> Connect to the host's network

Step 5: Assign IP address
        â””â”€> Give the Pod an IP (e.g., 10.244.1.5)

Step 6: Set up routing
        â””â”€> Make sure packets can reach the Pod
```

---

### **CNI Plugin Comparison:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FLANNEL                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Technology:  VXLAN overlay network                      â”‚
â”‚ Complexity:  Low (easiest to set up)                    â”‚
â”‚ Performance: Good                                       â”‚
â”‚ Features:    Basic networking only                      â”‚
â”‚ Use case:    Simple clusters, getting started           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CALICO                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Technology:  BGP routing + eBPF/iptables                â”‚
â”‚ Complexity:  Medium                                     â”‚
â”‚ Performance: Very Good                                  â”‚
â”‚ Features:    Rich NetworkPolicy support                 â”‚
â”‚ Use case:    Production clusters, need good policies    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CILIUM                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Technology:  Pure eBPF (kernel-native)                  â”‚
â”‚ Complexity:  Medium-High                                â”‚
â”‚ Performance: Best (O(1) lookups, no iptables)           â”‚
â”‚ Features:    L7 policies, identity-based, Hubble        â”‚
â”‚ Use case:    Large scale, need observability & speed    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7ï¸âƒ£ WHY eBPF? WHY CILIUM?

This is the key question from your whiteboard!

---

### **The Problem with Traditional Networking (iptables):**

```
        With 10,000 Pods in a cluster:
        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  iptables rule 1                        â”‚
â”‚  iptables rule 2                        â”‚
â”‚  iptables rule 3                        â”‚
â”‚  ...                                    â”‚
â”‚  iptables rule 9,997                    â”‚
â”‚  iptables rule 9,998                    â”‚
â”‚  iptables rule 9,999                    â”‚  â† Must scan EVERY rule
â”‚  iptables rule 10,000                   â”‚     for EVERY packet!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Every packet must scan ALL rules = O(n)
Slow! Gets worse with more Pods!
```

**Problem:**
- iptables uses a **linear list** of rules
- Every packet scans from top to bottom
- With thousands of Pods = thousands of rules
- Performance gets WORSE as cluster grows

---

### **The eBPF Solution:**

```
        With 10,000 Pods in a cluster:
        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚         eBPF Hash Map                   â”‚
â”‚                                         â”‚
â”‚    Key: Pod IP    â†’  Value: Rules      â”‚
â”‚    10.244.1.5     â†’  identity=1234     â”‚
â”‚    10.244.1.6     â†’  identity=5678     â”‚
â”‚    ...                                  â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Packet arrives â†’ Hash lookup â†’ O(1) constant time!
No matter if you have 10 Pods or 10,000 Pods!
```

**eBPF advantage:**
- Uses **hash tables** for lookups
- O(1) = constant time, doesn't matter how many Pods
- Dramatically faster at scale

---

### **What is eBPF:**

```
        Traditional Linux Kernel
        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Kernel Code                     â”‚
â”‚    (written in C, compiled in)          â”‚
â”‚                                         â”‚
â”‚    To change = recompile kernel         â”‚
â”‚                 or                      â”‚
â”‚    Write kernel module (risky!)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
        With eBPF
        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Kernel Code                     â”‚
â”‚    (original kernel untouched)          â”‚
â”‚                                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚    â”‚  eBPF Program 1     â”‚  â† You write this!
â”‚    â”‚  (runs in kernel)   â”‚  â† Safe, verified
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â† Hot-loaded
â”‚                                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚    â”‚  eBPF Program 2     â”‚             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**eBPF = Extended Berkeley Packet Filter**

A way to run custom programs **inside the Linux kernel** without:
- Recompiling the kernel
- Rebooting the system
- Risking kernel crashes

**How it works:**
1. Write a small program in C
2. Compile to eBPF bytecode
3. Load it into the kernel
4. Kernel **verifies** it's safe (no infinite loops, no bad memory access)
5. Kernel **JIT-compiles** it to native machine code
6. Runs at **near-native speed**

---

### **Where eBPF Hooks Into the Kernel:**

```
Packet Journey with eBPF:

Physical NIC
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  XDP Hook           â”‚ â† eBPF can attach here (earliest!)
â”‚  (eXpress Data Path)â”‚    Can drop packets instantly
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TC Hook            â”‚ â† eBPF can attach here
â”‚  (Traffic Control)  â”‚    Can modify, redirect packets
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Netfilter          â”‚ â† eBPF can attach here
â”‚  (iptables layer)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Socket Layer       â”‚ â† eBPF can attach here
â”‚                     â”‚    L7 inspection possible!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**eBPF can hook at MULTIPLE points** in the packet's journey:
- **XDP**: Earliest possible, before sk_buff created
- **TC**: After sk_buff, can modify packets
- **Netfilter**: Where iptables normally runs
- **Socket**: Application layer, can inspect HTTP/gRPC

---

## 8ï¸âƒ£ WHY CILIUM USES eBPF - THE REAL REASON

### **Problem 1: iptables is too slow**

```
Old way (kube-proxy with iptables):

Packet arrives
   â”‚
   â”œâ”€> Check iptables rule 1
   â”œâ”€> Check iptables rule 2
   â”œâ”€> Check iptables rule 3
   â”œâ”€> ...
   â”œâ”€> Check iptables rule 9,999
   â””â”€> Finally found the rule!
   
Time: O(n) - gets slower with more Pods
```

```
Cilium way (eBPF):

Packet arrives
   â”‚
   â””â”€> eBPF program does hash lookup
       Time: O(1) - always fast!
```

**Result:** Cilium can handle **10-100x more Pods** at the same speed.

---

### **Problem 2: Pod IPs keep changing**

```
Traditional iptables rules:

Rule: Allow traffic from 10.244.1.5 to 10.244.2.8

Problem:
- Pod restarts â†’ New IP: 10.244.1.9
- Old rule still says 10.244.1.5
- Rule is BROKEN!
```

```
Cilium way (Identity-based):

Rule: Allow traffic from identity=1234 to identity=5678

Where:
- identity=1234 = Pods with label app=frontend
- identity=5678 = Pods with label app=backend

When Pod restarts:
- Gets new IP, but SAME identity (labels don't change)
- Rule STILL WORKS!
```

**Result:** Policies survive Pod restarts. No need to update rules.

---

### **Problem 3: Can't see inside HTTP/gRPC**

```
Traditional firewalls:

Can only see:
- Source IP: 10.244.1.5
- Destination IP: 10.244.2.8
- Port: 80
- Protocol: TCP

Can't see:
- Which HTTP method? (GET, POST, DELETE)
- Which URL path? (/api/users, /api/admin)
- Which gRPC method?
```

```
Cilium way (L7 policies):

eBPF can see:
- HTTP method: POST
- URL: /api/users
- Headers
- Response codes

Can enforce:
"Allow only GET /api/public/*
 Block everything else"
```

**Result:** Application-layer security without a service mesh.

---

### **Problem 4: No visibility**

```
Traditional setup:

"Why is Pod A can't reach Pod B?"
"Is the traffic being blocked?"
"Which rule is dropping it?"

Answer: Â¯\_(ãƒ„)_/Â¯  (no easy way to see)
```

```
Cilium way (Hubble):

eBPF records EVERY packet:
- Source: default/frontend-xyz
- Destination: default/backend-abc
- Verdict: FORWARDED
- Identity: 1234 â†’ 5678
- L7: GET /api/users â†’ 200 OK

Real-time flow visibility!
```

**Result:** You can SEE what's happening in your cluster.

---

## 9ï¸âƒ£ THE COMPLETE FLOW - PUTTING IT ALL TOGETHER

Let me trace ONE packet's complete journey with ALL layers:

```
SENDER (outside world) wants to reach a Pod at 10.244.1.5

Step 1: Packet arrives at Physical NIC
        â”œâ”€> NIC writes to DMA buffer
        â””â”€> NIC fires interrupt

Step 2: Kernel receives packet
        â”œâ”€> Driver creates sk_buff
        â””â”€> Hands to netif_receive_skb()

Step 3: eBPF program runs (if Cilium installed)
        â”œâ”€> Attached at TC hook
        â”œâ”€> Checks identity-based policy
        â”œâ”€> Does NAT if needed (Service â†’ Pod IP)
        â””â”€> Decides: ALLOW or DROP

Step 4: Packet reaches bridge (br0 or OVS)
        â”œâ”€> Bridge looks at destination MAC
        â””â”€> Forwards to correct veth port

Step 5: Packet enters veth0 (host side)
        â””â”€> Magically appears at veth1 (container side)

Step 6: Packet is now in Pod's namespace
        â”œâ”€> veth1 is renamed to "eth0" inside Pod
        â”œâ”€> Pod's network stack processes it
        â””â”€> Application receives the data!

Throughout: Cilium's eBPF records the flow for Hubble
```

---

## ðŸŽ¯ SUMMARY - THE KEY POINTS

### **Physical NIC**
- Real hardware, the only actual network card
- All virtual NICs share this one card
- Uses DMA + IRQ to hand packets to kernel

### **Linux Kernel**
- Receives packet via interrupt
- Creates sk_buff (packet container)
- Entry point: `netif_receive_skb()`

### **Three Paths**
1. **PP (Packet Processing)**: Normal kernel stack - good for basic setups
2. **PB (DPDK)**: Bypass kernel - fastest, for extreme performance
3. **PF (eBPF)**: Programmable filter in kernel - modern, flexible

### **veth Pair**
- Virtual cable with two ends
- One end in host namespace, one in container namespace
- How containers get their network interface

### **Bridge / OVS**
- Software switch
- Forwards packets between veth ports
- Linux Bridge = simple, OVS = programmable

### **CNI**
- Standard for how to set up container networking
- Kubernetes calls CNI plugin to do the actual work
- Plugins: Flannel (simple), Calico (good), Cilium (best)

### **Why eBPF / Cilium**
1. **Speed**: O(1) hash lookups vs O(n) iptables
2. **Identity**: Label-based, survives Pod restarts
3. **L7**: Can see inside HTTP/gRPC traffic
4. **Observability**: Hubble shows real-time flows

---


